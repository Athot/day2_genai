{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('myenv/.env')\n",
    "a = os.getenv('find_me')\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key = a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Chat Openai\n",
    "llm=ChatOpenAI(\n",
    "    temperature=0.0,\n",
    "    model=llm_model\n",
    ")\n",
    "\n",
    "#2 Memory\n",
    "memory=ConversationBufferMemory()\n",
    "\n",
    "# ConversationChain \n",
    "conversation= ConversationChain(\n",
    "llm=llm,\n",
    "memory=memory,\n",
    "verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "athot_chat = ChatOpenAI(temperature=0.0, model=\"gpt-3.5-turbo\",openai_api_key = a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_style = \"\"\"\n",
    "    American English \\\n",
    "        in a rude tone\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_response = athot_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_string = \"\"\"Translate the text \\\n",
    "    that is delimitted by triple backticks \\\n",
    "    into a style that a {style}. \\\n",
    "    text: ((({text})))\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain Prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['style', 'text']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "prompt_template.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_email = '''\n",
    "Subject: Invitation to Lunch\n",
    "\n",
    "Hi [Friend's Name],\n",
    "\n",
    "I hope this email finds you well. I wanted to reach out and invite you to join me for lunch next week. It's been a while since we caught up, and I thought it would be great to spend some time together.\n",
    "\n",
    "I suggest meeting at [Restaurant Name] on [Date] at [Time]. Please let me know if this works for you, or if you have any other preferences. I'm flexible and open to suggestions.\n",
    "\n",
    "Looking forward to catching up and enjoying a delicious meal together!\n",
    "\n",
    "Best regards,\n",
    "[Your Name]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_messages = prompt_template.format_messages(\n",
    "    style = customer_style, \n",
    "    text = customer_email\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessage(content=\"Translate the text     that is delimitted by triple backticks     into a style that a \\n    American English         in a rude tone.     text: (((\\nSubject: Invitation to Lunch\\n\\nHi [Friend's Name],\\n\\nI hope this email finds you well. I wanted to reach out and invite you to join me for lunch next week. It's been a while since we caught up, and I thought it would be great to spend some time together.\\n\\nI suggest meeting at [Restaurant Name] on [Date] at [Time]. Please let me know if this works for you, or if you have any other preferences. I'm flexible and open to suggestions.\\n\\nLooking forward to catching up and enjoying a delicious meal together!\\n\\nBest regards,\\n[Your Name])))\\n    \")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(customer_messages)\n",
    "customer_messages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_response = athot_chat(customer_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: Invitation to Lunch\n",
      "\n",
      "Hey [Friend's Name],\n",
      "\n",
      "I hope you're not too busy to bother reading this email. I'm inviting you to join me for lunch next week, but honestly, I don't really care if you come or not. It's been a while since we caught up, and I thought it would be great to spend some time together, but if you're too busy, that's fine too.\n",
      "\n",
      "I suggest meeting at [Restaurant Name] on [Date] at [Time]. Let me know if this works for you, or if you have any other preferences. But honestly, I don't really care where we eat, I just thought I'd throw out a suggestion.\n",
      "\n",
      "Looking forward to catching up and enjoying a delicious meal together, but if you can't make it, no big deal.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n"
     ]
    }
   ],
   "source": [
    "print(customer_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': False, 'delivery_days': 5, 'price_value': 'pretty affortable'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    \"gift\" : False,\n",
    "    \"delivery_days\" : 5,\n",
    "    \"price_value\" : \"pretty affortable\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review = \"\"\"\\\n",
    "This leaf blower is pretty amazing. It has four settings:\\\n",
    "candle bower, gentle breeze, windy city, and tornado.\\\n",
    "It arrived in two days, just for my wife\\\n",
    "anniversery present.\\\n",
    "I think my wife liked it so much she was speecheless\\\n",
    "So far I've been the only one using it, and I have been \\\n",
    "using it every other morning than other leaves on our lawn\\\n",
    "using it every other morning than the other leaf blowwerss\\\n",
    "It's slightly more expensive than the other leaf blowers,\\\n",
    "out there, but I think it's worth it for extra features\"\"\"\n",
    "\n",
    "review_templates = \"\"\"\\\n",
    "For the following text, extract the following informations:\n",
    "\n",
    "gift: Was the item purchased as a gift of someone else?\\\n",
    "Anser Ture if yes, False if not unknown.\n",
    "    \n",
    "    \n",
    "price_value : Extract any sentences about the value or the prices,\\\n",
    "and output them as a comma seperated python list.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "\n",
    "text: {text}\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "prompt_template = ChatPromptTemplate.from_template(review_templates)\n",
    "prompt_template.messages[0].prompt.input_variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_messages2 = prompt_template.format_messages(\n",
    "   \n",
    "    text = customer_review\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'athot_chat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m customer_response \u001b[38;5;241m=\u001b[39m \u001b[43mathot_chat\u001b[49m(customer_messages2)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(customer_response\u001b[38;5;241m.\u001b[39mcontent)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'athot_chat' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "customer_response = athot_chat(customer_messages2)\n",
    "print(customer_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "a = (7,0,8,0,0, 9)\n",
    "# to count the number of 0\n",
    "a.count(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# demo practice on langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import ChatOpenAI\n",
    "# import os\n",
    "\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv(\"myenv2/.env\")\n",
    "# a = os.environ.get('find_me')\n",
    "# chat = ChatOpenAI(openai_api_key = a)\n",
    "import os\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "a = os.environ.get('find_me')\n",
    "# chat = ChatOpenAI(openai_api_key=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content = \"You are a helpful assistant\"),\n",
    "    HumanMessage(content=\"What is the purpose of model regularization short defination\")\n",
    "]\n",
    "\n",
    "chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "a = os.getenv('find_me')\n",
    "os.environ['OPENAI_API_KEY'] = a\n",
    "# print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(temperature=0.6)\n",
    "name = llm(\"I want to open a restuarant for Indian food. Suggest a fancy name for this\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I want to open a restuarant for Chinese Italian food. Suggest a fancy name for this'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables=['cuisine'],\n",
    "    template=\"I want to open a restuarant for {cuisine} Italian food. Suggest a fancy name for this\"\n",
    ")\n",
    "prompt_template_name.format(cuisine = \"Chinese\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\"Dragon\\'s Table: A Fusion of Chinese and Italian Cuisine\"'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm= llm, prompt=prompt_template_name)\n",
    "chain.run(\"China\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple sequential chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.6)\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables=['cuisine'],\n",
    "    template=\"I want to open a restaurant for {cuisine} food. Suggest a fancy for this\"\n",
    ")\n",
    "\n",
    "name_chain = LLMChain(llm=llm, prompt=prompt_template_name)\n",
    "\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables=['restuarant_name'],\n",
    "    template=\"\"\"Suggest some menu items {restaurant_name}. Return it in a comma seperately\"\"\"\n",
    ")\n",
    "food_items_chain = LLMChain(llm=llm, prompt=prompt_template_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "chain = SimpleSequentialChain(chains=[name_chain, food_items_chain])\n",
    "response = chain.run(\"Indian\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.6)\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables=['cuisine'],\n",
    "    template=\"I want to open a restaurant for {cuisine} food. Suggest a fancy for this in a new line\"\n",
    ")\n",
    "\n",
    "name_chain = LLMChain(llm=llm, prompt=prompt_template_name, output_key = \"restaurant_name\")\n",
    "\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables=['restuarant_name'],\n",
    "    template=\"\"\"Suggest some menu items {restaurant_name}. Return it in a comma seperately in a new line\"\"\"\n",
    ")\n",
    "food_items_chain = LLMChain(llm=llm, prompt=prompt_template_items, output_key=\"menu_items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cuisine': 'Arabic',\n",
       " 'restaurant_name': '\\n\\n\"Al-Majlis: A Luxurious Taste of Arabia\" ',\n",
       " 'menu_items': '\\n\\n1. Appetizers:\\n- Hummus with warm pita bread\\n- Falafel bites with tahini dipping sauce\\n- Tabouleh salad\\n- Baba ganoush with crispy pita chips\\n- Stuffed grape leaves\\n- Fattoush salad\\n\\n2. Entrees:\\n- Lamb kabobs with grilled vegetables\\n- Chicken shawarma wrap\\n- Beef shawarma platter with rice and salad\\n- Vegetable biryani\\n- Shrimp kabsa with saffron rice\\n- Mixed grill platter (lamb, chicken, and beef)\\n\\n3. Sides:\\n- Basmati rice\\n- Grilled vegetables\\n- Garlic naan bread\\n- Lentil soup\\n- Arabic pickles\\n- Fried cauliflower with tahini sauce\\n\\n4. Desserts:\\n- Baklava\\n- Umm Ali (Egyptian bread pudding)\\n- Kunafa (sweet cheese pastry)\\n- Halawet el-jibn (creamy cheese rolls)\\n- Kanafeh (shredded phyllo pastry with cheese)\\n- Cardamom-infused rice pudding\\n\\n5. Beverages:\\n- Arabic coffee\\n- Mint lemonade\\n- Jallab (date syrup drink)\\n- Mango lassi\\n-'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "chain = SequentialChain(\n",
    "    chains=[name_chain, food_items_chain],\n",
    "    input_variables=['cuisine'],\n",
    "    output_variables=['restaurant_name', 'menu_items']\n",
    ")\n",
    "chain({'cuisine':'Arabic'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m To answer this question, I should use wikipedia to find out Elon Musk's birth date and then use the calculator to determine his age in 2023.\n",
      "Action: wikipedia\n",
      "Action Input: Elon Musk\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Elon Musk\n",
      "Summary: Elon Reeve Musk (; EE-lon; born June 28, 1971) is a businessman and investor. He is the founder, chairman, CEO, and CTO of SpaceX; angel investor, CEO, product architect, and former chairman of Tesla, Inc.; owner, executive chairman, and CTO of X Corp.; founder of the Boring Company and xAI; co-founder of Neuralink and OpenAI; and president of the Musk Foundation. He is one of the wealthiest people in the world, with an estimated net worth of US$190 billion as of March 2024, according to the Bloomberg Billionaires Index, and $195 billion according to Forbes, primarily from his ownership stakes in Tesla and SpaceX.A member of the wealthy South African Musk family, Elon was born in Pretoria and briefly attended the University of Pretoria before immigrating to Canada at age 18, acquiring citizenship through his Canadian-born mother. Two years later, he matriculated at Queen's University at Kingston in Canada. Musk later transferred to the University of Pennsylvania, and received bachelor's degrees in economics and physics. He moved to California in 1995 to attend Stanford University, but dropped out after two days and, with his brother Kimbal, co-founded online city guide software company Zip2. The startup was acquired by Compaq for $307 million in 1999, and that same year Musk co-founded X.com, a direct bank. X.com merged with Confinity in 2000 to form PayPal.\n",
      "In October 2002, eBay acquired PayPal for $1.5 billion, and that same year, with $100 million of the money he made, Musk founded SpaceX, a spaceflight services company. In 2004, he became an early investor in electric vehicle manufacturer Tesla Motors, Inc. (now Tesla, Inc.). He became its chairman and product architect, assuming the position of CEO in 2008. In 2006, Musk helped create SolarCity, a solar-energy company that was acquired by Tesla in 2016 and became Tesla Energy. In 2013, he proposed a hyperloop high-speed vactrain transportation system. In 2015, he co-founded OpenAI, a nonprofit artificial intelligence research company. The following year, Musk co-founded Neuralink—a neurotechnology company developing brain–computer interfaces—and the Boring Company, a tunnel construction company. In 2022, he acquired Twitter for $44 billion. He subsequently merged the company into newly created X Corp. and rebranded the service as X the following year. In March 2023, he founded xAI, an artificial intelligence company.\n",
      "Musk has expressed views that have made him a polarizing figure. He has been criticized for making unscientific and misleading statements, including COVID-19 misinformation and antisemitic conspiracy theories. His ownership of Twitter has been similarly controversial, being marked by the laying off of a large number of employees, an increase in hate speech and misinformation and disinformation on the website, as well as changes to Twitter Blue verification. In 2018, the U.S. Securities and Exchange Commission (SEC) sued him, alleging that he had falsely announced that he had secured funding for a private takeover of Tesla. To settle the case, Musk stepped down as the chairman of Tesla and paid a $20 million fine.\n",
      "\n",
      "Page: Twitter under Elon Musk\n",
      "Summary: Elon Musk completed his acquisition of Twitter in October 2022; Musk acted as CEO of Twitter until June 2023 when he was succeeded by Linda Yaccarino. Twitter was then rebranded to X in July 2023. Initially during Musk's tenure, Twitter introduced a series of reforms and management changes; the company reinstated a number of previously banned accounts, reduced the workforce by approximately 80%, closed one of Twitter's three data centers, and largely eliminated the content moderation team, replacing it with the crowd-sourced fact-checking system Community Notes.\n",
      "In November 2022, Twitter then began offering paid verification checkmarks, followed by removing legacy verification. In December, the Twitter Files were released and a number of journalists suspended from the platform. The foll\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now have Elon Musk's birth date and information about his career. I can use the calculator to determine his age in 2023.\n",
      "Action: Calculator\n",
      "Action Input: 2023 - 1971\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 52\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: Elon Musk will be 52 years old in 2023.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Elon Musk will be 52 years old in 2023.'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose = True\n",
    ")\n",
    "agent.run(\"When was Elon musk born? What is his age in 2023\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"La Cantina de México: A Modern Twist on Traditional Flavors\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory =ConversationBufferMemory()\n",
    "chain = LLMChain(llm = llm, prompt = prompt_template_name, memory=memory)\n",
    "name = chain.run(\"Mexico\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Mexico\n",
      "AI: \n",
      "\n",
      "\"La Cantina de México: A Modern Twist on Traditional Flavors\"\n"
     ]
    }
   ],
   "source": [
    "print(chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "convo = ConversationChain(llm =  OpenAI(temperature = 0.3))\n",
    "print(convo.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "convo = ConversationChain(llm = OpenAI(temperature=0.6), memory= memory)\n",
    "memory = ConversationBufferWindowMemory(k = 1)\n",
    "convo.run(\"Who is the best baskeetball player\")\n",
    "print(convo.prompt.template)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
